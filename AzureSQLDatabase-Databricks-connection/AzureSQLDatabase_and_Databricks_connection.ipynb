{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4326aa0-d3a3-4788-9781-72be9ead1985",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# prerequisits:\n",
    "# add your local and databricks ip address to Azure SQL database firewall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9de14e51-5cc6-47f7-825f-f77f94a44bcf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### METHOD 1: Using jdbc url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d26b14-68d6-4889-bb90-7cdbf0053675",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Host name is server name in Azure SQL overview page\n",
    "jdbcHostname = \"<server_name>.database.windows.net\"\n",
    "\n",
    "# jdbc default port number is 1433\n",
    "jdbcPort = 1433\n",
    "\n",
    "# enter your database name (which we can find from our resource group resources)\n",
    "jdbcDatabase = \"<database_name>\"\n",
    "\n",
    "# enter your username\n",
    "jdbcUsername = \"<database_username>\"\n",
    "\n",
    "# enter the password of your database\n",
    "jdbcPassword = \"<password>\"\n",
    "\n",
    "# common driver to connect to SQLserver in microsoft\n",
    "jdbcDriver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "\n",
    "# final string specifying all the parameters\n",
    "jdbcUrl = f\"jdbc:sqlserver://{jdbcHostname}:{jdbcPort};databaseName={jdbcDatabase};user={jdbcUsername};password={jdbcPassword}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2b95930-8558-4c76-8070-93d86b926938",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- ProductID: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- ProductNumber: string (nullable = true)\n |-- Color: string (nullable = true)\n |-- StandardCost: decimal(19,4) (nullable = true)\n |-- ListPrice: decimal(19,4) (nullable = true)\n |-- Size: string (nullable = true)\n |-- Weight: decimal(8,2) (nullable = true)\n |-- ProductCategoryID: integer (nullable = true)\n |-- ProductModelID: integer (nullable = true)\n |-- SellStartDate: timestamp (nullable = true)\n |-- SellEndDate: timestamp (nullable = true)\n |-- DiscontinuedDate: timestamp (nullable = true)\n |-- ThumbNailPhoto: binary (nullable = true)\n |-- ThumbnailPhotoFileName: string (nullable = true)\n |-- rowguid: string (nullable = true)\n |-- ModifiedDate: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# using spark.read we can read the data from database. We have to give database.table details in option along with\n",
    "# url details and format of read.\n",
    "# change .option('dbtable',<schema_name>.<table_name>)\n",
    "\n",
    "df = spark.read.format('jdbc').option('url',jdbcUrl).option('dbtable','SalesLT.Product').load()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbce6d2f-de9f-4e0e-8582-c952f8279bca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "###### Note: If IP address of Databricks is not entered in Azure SQL firewall, then the above command will give an error specifying the IP address. So assign the IP address of databricks workspace in Azure SQL firewall as well to give access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93191ece-c0dc-4445-8193-77c8bfc73d24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "##### METHOD 2: Using connection string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c88d170-d5a1-432f-b8ac-e36e81f0e8d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# to get the connection string navigate to Azure SQL database resource overview page >> show databases connection string >> JDBC >> <copy the connection string and past is here>\n",
    "\n",
    "# The connection string by default doesnt come with password. So edit password in connection string\n",
    "\n",
    "connectionString = \"<connection_string>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70ae417c-ae7b-4135-bb31-afe72d1b17ac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# use spark.read to read the database using jdbc option to specify the connection string and table details\n",
    "# change .jdbc(connectionString,'<schema_name>.<table_name>')\n",
    "\n",
    "df2 = spark.read.jdbc(connectionString,'SalesLT.ProductCategory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e059ed2-b13d-4041-8d89-f7dd7bf54575",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- ProductCategoryID: integer (nullable = true)\n |-- ParentProductCategoryID: integer (nullable = true)\n |-- Name: string (nullable = true)\n |-- rowguid: string (nullable = true)\n |-- ModifiedDate: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a3a6853-1ca2-48d6-a10e-b0429c48b57d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "AzureSQLDatabase_and_Databricks_connection",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
